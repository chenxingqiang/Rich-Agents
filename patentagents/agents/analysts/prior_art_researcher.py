"""
Prior Art Researcher Agent - å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜æ™ºèƒ½ä½“
æ·±åº¦æ£€ç´¢ç›¸å…³ä¸“åˆ©ã€æŠ€æœ¯æ–‡çŒ®ï¼Œè¯„ä¼°ç°æœ‰æŠ€æœ¯çŠ¶æ€
"""

import functools
import json
import logging
from typing import Dict, Any, List, Tuple, Set
from datetime import datetime, timedelta
from collections import defaultdict

from langchain_core.messages import HumanMessage, AIMessage

logger = logging.getLogger(__name__)


def create_prior_art_researcher(llm, toolkit):
    """
    åˆ›å»ºå…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜æ™ºèƒ½ä½“
    
    Args:
        llm: è¯­è¨€æ¨¡å‹å®ä¾‹
        toolkit: ä¸“åˆ©å·¥å…·åŒ…å®ä¾‹
    
    Returns:
        function: å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜èŠ‚ç‚¹å‡½æ•°
    """
    
    def prior_art_researcher_node(state):
        """
        å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜èŠ‚ç‚¹ - æ·±åº¦æ£€ç´¢å’Œåˆ†æå…ˆè¡ŒæŠ€æœ¯
        
        Args:
            state: ä¸“åˆ©åˆ†æçŠ¶æ€
            
        Returns:
            dict: æ›´æ–°åçš„çŠ¶æ€
        """
        technology_domain = state["technology_domain"]
        innovation_topic = state["innovation_topic"]
        analysis_date = state["analysis_date"]
        
        # è·å–å‰é¢æ™ºèƒ½ä½“çš„åˆ†æç»“æœ
        technology_report = state.get("technology_report", "")
        innovation_opportunities = state.get("innovation_opportunities", "")
        
        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜ï¼Œä¸“é—¨è¿›è¡Œæ·±åº¦çš„ä¸“åˆ©æ£€ç´¢å’Œç°æœ‰æŠ€æœ¯åˆ†æã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š

1. **å…¨é¢ä¸“åˆ©æ£€ç´¢**ï¼š
   - è¿›è¡Œå¤šç»´åº¦ã€å¤šå…³é”®è¯çš„ä¸“åˆ©æ£€ç´¢
   - è¦†ç›–å…¨çƒä¸»è¦ä¸“åˆ©æ•°æ®åº“
   - æ£€ç´¢ä¸åŒæ—¶é—´æ®µçš„ä¸“åˆ©å‘å±•
   - åˆ†æä¸“åˆ©æ—å’Œä¸“åˆ©å¼•ç”¨å…³ç³»

2. **ç°æœ‰æŠ€æœ¯åˆ†æ**ï¼š
   - è¯†åˆ«æ ¸å¿ƒå…ˆè¡ŒæŠ€æœ¯å’ŒåŸºç¡€ä¸“åˆ©
   - åˆ†ææŠ€æœ¯å‘å±•è„‰ç»œå’Œæ¼”è¿›è·¯å¾„
   - è¯„ä¼°ç°æœ‰æŠ€æœ¯çš„è¦†ç›–èŒƒå›´å’Œå±€é™æ€§
   - è¯†åˆ«æŠ€æœ¯ç©ºç™½å’Œæ”¹è¿›æœºä¼š

3. **ä¸“åˆ©åœ°å›¾æ„å»º**ï¼š
   - æ„å»ºæŠ€æœ¯é¢†åŸŸçš„ä¸“åˆ©åœ°å›¾
   - åˆ†æä¸»è¦ä¸“åˆ©æƒäººå’Œå‘æ˜äºº
   - è¯†åˆ«æ ¸å¿ƒä¸“åˆ©å’ŒåŸºç¡€ä¸“åˆ©
   - è¯„ä¼°ä¸“åˆ©å¼ºåº¦å’Œä»·å€¼

4. **ä¾µæƒé£é™©è¯„ä¼°**ï¼š
   - è¯†åˆ«å¯èƒ½çš„ä¾µæƒé£é™©ä¸“åˆ©
   - åˆ†æä¸“åˆ©æƒåˆ©è¦æ±‚çš„è¦†ç›–èŒƒå›´
   - è¯„ä¼°ä¸“åˆ©æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§
   - æä¾›è§„é¿è®¾è®¡å»ºè®®

5. **æŠ€æœ¯å‘å±•è¶‹åŠ¿**ï¼š
   - åˆ†æä¸“åˆ©ç”³è¯·è¶‹åŠ¿å’ŒæŠ€æœ¯çƒ­ç‚¹
   - è¯†åˆ«æ–°å…´æŠ€æœ¯æ–¹å‘
   - é¢„æµ‹æŠ€æœ¯å‘å±•è·¯å¾„
   - è¯„ä¼°æŠ€æœ¯æˆç†Ÿåº¦

ä½ éœ€è¦ä½¿ç”¨ä¸“åˆ©æ£€ç´¢å·¥å…·è¿›è¡Œå…¨é¢çš„æ£€ç´¢ï¼Œå¹¶åŸºäºæ£€ç´¢ç»“æœè¿›è¡Œæ·±å…¥åˆ†æã€‚

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨ç»“æ„åŒ–çš„markdownæ ¼å¼
- åŒ…å«è¯¦ç»†çš„æ£€ç´¢ç­–ç•¥å’Œç»“æœ
- æä¾›ä¸“åˆ©åˆ†æè¡¨æ ¼å’Œå›¾è¡¨
- åŒ…å«é£é™©è¯„ä¼°å’Œå»ºè®®
- åœ¨æŠ¥å‘Šæœ«å°¾æ·»åŠ å…³é”®ä¸“åˆ©æ¸…å•
"""
        
        # æ„å»ºç”¨æˆ·è¾“å…¥
        user_input = f"""è¯·å¯¹ä»¥ä¸‹æŠ€æœ¯é¢†åŸŸè¿›è¡Œå…¨é¢çš„å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶ï¼š

**æŠ€æœ¯é¢†åŸŸ**: {technology_domain}
**å…·ä½“æ–¹å‘**: {innovation_topic}
**åˆ†ææ—¥æœŸ**: {analysis_date}

**æŠ€æœ¯åˆ†æèƒŒæ™¯**ï¼š
{technology_report[:800] if technology_report else "æš‚æ— æŠ€æœ¯åˆ†æèƒŒæ™¯"}

**åˆ›æ–°æœºä¼šèƒŒæ™¯**ï¼š
{innovation_opportunities[:800] if innovation_opportunities else "æš‚æ— åˆ›æ–°æœºä¼šèƒŒæ™¯"}

è¯·è¿›è¡Œæ·±åº¦çš„ä¸“åˆ©æ£€ç´¢å’Œç°æœ‰æŠ€æœ¯åˆ†æã€‚"""
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]
        
        # å·¥å…·è°ƒç”¨ï¼šè¿›è¡Œå…¨é¢çš„ä¸“åˆ©æ£€ç´¢
        try:
            logger.info(f"å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜å¼€å§‹åˆ†æ: {technology_domain} - {innovation_topic}")
            
            # 1. å¤šç»´åº¦ä¸“åˆ©æ£€ç´¢
            search_results = _comprehensive_patent_search(toolkit, technology_domain, innovation_topic)
            
            # 2. ä¸“åˆ©å¼•ç”¨åˆ†æ
            citation_analysis = _analyze_patent_citations(toolkit, search_results["key_patents"])
            
            # 3. ä¸“åˆ©æ—åˆ†æ
            patent_family_analysis = _analyze_patent_families(toolkit, search_results["key_patents"])
            
            # 4. ç«äº‰å¯¹æ‰‹åˆ†æ
            competitor_analysis = _analyze_competitors(toolkit, search_results["patents"])
            
            # 5. æŠ€æœ¯æ¼”è¿›åˆ†æ
            technology_evolution = _analyze_technology_evolution(toolkit, search_results["patents"])
            
            # 6. ä¾µæƒé£é™©è¯„ä¼°
            infringement_risks = _assess_infringement_risks(toolkit, search_results["high_risk_patents"])
            
            # æ•´ç†å…ˆè¡ŒæŠ€æœ¯åˆ†ææ•°æ®
            prior_art_data = {
                "search_results": search_results,
                "citation_analysis": citation_analysis,
                "patent_families": patent_family_analysis,
                "competitors": competitor_analysis,
                "technology_evolution": technology_evolution,
                "infringement_risks": infringement_risks
            }
            
            # å¢å¼ºç”¨æˆ·è¾“å…¥
            enhanced_user_input = f"""{user_input}

**å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶æ•°æ®**ï¼š

1. **ä¸“åˆ©æ£€ç´¢ç»“æœ**ï¼š
   - æ€»ä¸“åˆ©æ•°ï¼š{search_results['total_patents']}
   - æ ¸å¿ƒä¸“åˆ©æ•°ï¼š{len(search_results['key_patents'])}
   - é«˜é£é™©ä¸“åˆ©æ•°ï¼š{len(search_results['high_risk_patents'])}
   - æ£€ç´¢ç­–ç•¥ï¼š{json.dumps(search_results['search_strategy'], ensure_ascii=False, indent=2)}

2. **ä¸“åˆ©å¼•ç”¨åˆ†æ**ï¼š
   {json.dumps(citation_analysis, ensure_ascii=False, indent=2)}

3. **ä¸“åˆ©æ—åˆ†æ**ï¼š
   {json.dumps(patent_family_analysis, ensure_ascii=False, indent=2)}

4. **ç«äº‰å¯¹æ‰‹åˆ†æ**ï¼š
   {json.dumps(competitor_analysis, ensure_ascii=False, indent=2)}

5. **æŠ€æœ¯æ¼”è¿›åˆ†æ**ï¼š
   {json.dumps(technology_evolution, ensure_ascii=False, indent=2)}

6. **ä¾µæƒé£é™©è¯„ä¼°**ï¼š
   {json.dumps(infringement_risks, ensure_ascii=False, indent=2)}

è¯·åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œæ·±å…¥çš„å…ˆè¡ŒæŠ€æœ¯åˆ†æã€‚"""
            
            messages[1]["content"] = enhanced_user_input
            
        except Exception as e:
            logger.error(f"å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜æ•°æ®è·å–å¤±è´¥: {str(e)}")
            error_msg = f"\n\n**æ³¨æ„**: å…ˆè¡ŒæŠ€æœ¯æ•°æ®è·å–å¤±è´¥({str(e)})ï¼Œå°†åŸºäºé¢†åŸŸçŸ¥è¯†è¿›è¡Œåˆ†æã€‚"
            messages[1]["content"] += error_msg
            prior_art_data = {}
        
        # è°ƒç”¨LLMè¿›è¡Œå…ˆè¡ŒæŠ€æœ¯åˆ†æ
        try:
            result = llm.invoke(messages)
            
            # ç”Ÿæˆå…ˆè¡ŒæŠ€æœ¯æŠ¥å‘Š
            prior_art_report = result.content
            
            # æå–å…³é”®ä¿¡æ¯
            key_patents = prior_art_data.get("search_results", {}).get("key_patents", [])
            similar_patents = prior_art_data.get("search_results", {}).get("similar_patents", [])
            
            # æ›´æ–°çŠ¶æ€
            updated_state = {
                "messages": [result],
                "prior_art_report": prior_art_report,
                "sender": "Prior Art Researcher",
                "patent_search_results": prior_art_data.get("search_results", {}).get("patents", []),
                "similar_patents": similar_patents,
                "patent_citations": prior_art_data.get("citation_analysis", {}),
                "infringement_risk": prior_art_data.get("infringement_risks", {}),
                "competitive_landscape": prior_art_data.get("competitors", {})
            }
            
            logger.info(f"å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(key_patents)} ä¸ªæ ¸å¿ƒä¸“åˆ©")
            return updated_state
            
        except Exception as e:
            logger.error(f"å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            
            # ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
            error_report = f"""# å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶æŠ¥å‘Š

## âŒ ç ”ç©¶å¤±è´¥

**æŠ€æœ¯é¢†åŸŸ**: {technology_domain}
**å…·ä½“æ–¹å‘**: {innovation_topic}
**åˆ†ææ—¥æœŸ**: {analysis_date}

**é”™è¯¯ä¿¡æ¯**: {str(e)}

**å»ºè®®**: è¯·æ£€æŸ¥LLMé…ç½®æˆ–ç¨åé‡è¯•ã€‚"""
            
            return {
                "messages": [AIMessage(content=error_report)],
                "prior_art_report": error_report,
                "sender": "Prior Art Researcher",
                "patent_search_results": [],
                "similar_patents": [],
                "patent_citations": {},
                "infringement_risk": {},
                "competitive_landscape": {}
            }
    
    return functools.partial(prior_art_researcher_node, name="Prior Art Researcher")


def _comprehensive_patent_search(toolkit, technology_domain: str, innovation_topic: str) -> Dict[str, Any]:
    """
    è¿›è¡Œå…¨é¢çš„ä¸“åˆ©æ£€ç´¢
    
    Args:
        toolkit: ä¸“åˆ©å·¥å…·åŒ…
        technology_domain: æŠ€æœ¯é¢†åŸŸ
        innovation_topic: åˆ›æ–°ä¸»é¢˜
        
    Returns:
        Dict: æ£€ç´¢ç»“æœ
    """
    search_results = {
        "patents": [],
        "key_patents": [],
        "high_risk_patents": [],
        "similar_patents": [],
        "total_patents": 0,
        "search_strategy": []
    }
    
    try:
        # 1. åŸºç¡€å…³é”®è¯æœç´¢
        base_query = f"{technology_domain} {innovation_topic}"
        base_results = toolkit.search_google_patents(query=base_query, num=50)
        
        search_results["patents"].extend(base_results.get("patents", []))
        search_results["search_strategy"].append({
            "query": base_query,
            "results": len(base_results.get("patents", []))
        })
        
        # 2. æ‰©å±•å…³é”®è¯æœç´¢
        extended_queries = _generate_extended_queries(technology_domain, innovation_topic)
        
        for query in extended_queries:
            extended_results = toolkit.search_google_patents(query=query, num=30)
            search_results["patents"].extend(extended_results.get("patents", []))
            search_results["search_strategy"].append({
                "query": query,
                "results": len(extended_results.get("patents", []))
            })
        
        # 3. å»é‡å’Œåˆ†ç±»
        unique_patents = _deduplicate_patents(search_results["patents"])
        search_results["patents"] = unique_patents
        search_results["total_patents"] = len(unique_patents)
        
        # 4. è¯†åˆ«æ ¸å¿ƒä¸“åˆ©
        search_results["key_patents"] = _identify_key_patents(unique_patents)
        
        # 5. è¯†åˆ«é«˜é£é™©ä¸“åˆ©
        search_results["high_risk_patents"] = _identify_high_risk_patents(unique_patents)
        
        # 6. å¯»æ‰¾ç›¸ä¼¼ä¸“åˆ©
        if search_results["key_patents"]:
            first_key_patent = search_results["key_patents"][0]
            similar_patents = toolkit.search_similar_patents(
                first_key_patent.get("patent_id", ""), 
                threshold=0.7
            )
            search_results["similar_patents"] = similar_patents
        
        return search_results
        
    except Exception as e:
        logger.error(f"å…¨é¢ä¸“åˆ©æ£€ç´¢å¤±è´¥: {str(e)}")
        return search_results


def _generate_extended_queries(technology_domain: str, innovation_topic: str) -> List[str]:
    """ç”Ÿæˆæ‰©å±•çš„æ£€ç´¢æŸ¥è¯¢"""
    queries = []
    
    # åŸºäºæŠ€æœ¯é¢†åŸŸçš„æ‰©å±•
    domain_extensions = {
        "äººå·¥æ™ºèƒ½": ["AI", "machine learning", "deep learning", "neural network"],
        "ç”Ÿç‰©æŠ€æœ¯": ["biotechnology", "genetic engineering", "bioengineering"],
        "æ–°èƒ½æº": ["renewable energy", "clean energy", "sustainable energy"],
        "åŒºå—é“¾": ["blockchain", "distributed ledger", "cryptocurrency"],
        "ç‰©è”ç½‘": ["IoT", "Internet of Things", "connected devices"],
        "é‡å­è®¡ç®—": ["quantum computing", "quantum algorithm", "quantum information"]
    }
    
    # æ·»åŠ é¢†åŸŸæ‰©å±•æŸ¥è¯¢
    for domain, extensions in domain_extensions.items():
        if domain in technology_domain:
            for ext in extensions:
                queries.append(f"{ext} {innovation_topic}")
    
    # æ·»åŠ é€šç”¨æŠ€æœ¯æœ¯è¯­ç»„åˆ
    technical_terms = ["method", "system", "apparatus", "device", "process"]
    for term in technical_terms:
        queries.append(f"{innovation_topic} {term}")
    
    return queries[:5]  # é™åˆ¶æŸ¥è¯¢æ•°é‡


def _deduplicate_patents(patents: List[Dict]) -> List[Dict]:
    """å»é™¤é‡å¤ä¸“åˆ©"""
    seen_ids = set()
    unique_patents = []
    
    for patent in patents:
        patent_id = patent.get("patent_id", "")
        publication_number = patent.get("publication_number", "")
        
        # ä½¿ç”¨ä¸“åˆ©IDæˆ–å…¬å¼€å·ä½œä¸ºå”¯ä¸€æ ‡è¯†
        identifier = patent_id or publication_number
        
        if identifier and identifier not in seen_ids:
            seen_ids.add(identifier)
            unique_patents.append(patent)
    
    return unique_patents


def _identify_key_patents(patents: List[Dict]) -> List[Dict]:
    """è¯†åˆ«æ ¸å¿ƒä¸“åˆ©"""
    key_patents = []
    
    for patent in patents:
        # åŸºäºå¤šä¸ªå› ç´ è¯„ä¼°ä¸“åˆ©é‡è¦æ€§
        importance_score = 0
        
        # 1. å‘å¸ƒæ—¥æœŸï¼ˆè¾ƒæ–°çš„ä¸“åˆ©æ›´é‡è¦ï¼‰
        pub_date = patent.get("publication_date", "")
        if pub_date:
            try:
                pub_year = int(pub_date[:4])
                current_year = datetime.now().year
                if current_year - pub_year <= 5:
                    importance_score += 2
                elif current_year - pub_year <= 10:
                    importance_score += 1
            except:
                pass
        
        # 2. å—è®©äººï¼ˆçŸ¥åå…¬å¸çš„ä¸“åˆ©æ›´é‡è¦ï¼‰
        assignee = patent.get("assignee", "").lower()
        important_assignees = ["google", "microsoft", "apple", "ibm", "samsung", "intel", "qualcomm"]
        if any(company in assignee for company in important_assignees):
            importance_score += 2
        
        # 3. æ ‡é¢˜ç›¸å…³æ€§
        title = patent.get("title", "").lower()
        if len(title) > 50:  # è¯¦ç»†çš„æ ‡é¢˜é€šå¸¸æ›´é‡è¦
            importance_score += 1
        
        # 4. ä¸“åˆ©çŠ¶æ€
        if patent.get("status") == "GRANT":
            importance_score += 1
        
        # æ·»åŠ è¯„åˆ†
        patent["importance_score"] = importance_score
        
        # é€‰æ‹©é«˜åˆ†ä¸“åˆ©ä½œä¸ºæ ¸å¿ƒä¸“åˆ©
        if importance_score >= 3:
            key_patents.append(patent)
    
    # æŒ‰é‡è¦æ€§æ’åºå¹¶è¿”å›å‰20ä¸ª
    key_patents.sort(key=lambda x: x.get("importance_score", 0), reverse=True)
    return key_patents[:20]


def _identify_high_risk_patents(patents: List[Dict]) -> List[Dict]:
    """è¯†åˆ«é«˜é£é™©ä¸“åˆ©"""
    high_risk_patents = []
    
    for patent in patents:
        risk_score = 0
        
        # 1. æƒåˆ©è¦æ±‚èŒƒå›´å¹¿æ³›
        title = patent.get("title", "").lower()
        snippet = patent.get("snippet", "").lower()
        
        broad_terms = ["system", "method", "apparatus", "device", "process"]
        if any(term in title for term in broad_terms):
            risk_score += 1
        
        # 2. æˆæƒä¸“åˆ©é£é™©æ›´é«˜
        if patent.get("status") == "GRANT":
            risk_score += 2
        
        # 3. è¿‘æœŸä¸“åˆ©é£é™©æ›´é«˜
        pub_date = patent.get("publication_date", "")
        if pub_date:
            try:
                pub_year = int(pub_date[:4])
                current_year = datetime.now().year
                if current_year - pub_year <= 3:
                    risk_score += 2
            except:
                pass
        
        # 4. å¤§å…¬å¸ä¸“åˆ©é£é™©æ›´é«˜
        assignee = patent.get("assignee", "").lower()
        major_companies = ["google", "microsoft", "apple", "ibm", "samsung"]
        if any(company in assignee for company in major_companies):
            risk_score += 1
        
        patent["risk_score"] = risk_score
        
        if risk_score >= 4:
            high_risk_patents.append(patent)
    
    # æŒ‰é£é™©æ’åº
    high_risk_patents.sort(key=lambda x: x.get("risk_score", 0), reverse=True)
    return high_risk_patents[:15]


def _analyze_patent_citations(toolkit, key_patents: List[Dict]) -> Dict[str, Any]:
    """åˆ†æä¸“åˆ©å¼•ç”¨å…³ç³»"""
    citation_analysis = {
        "total_citations": 0,
        "avg_citations": 0,
        "citation_network": {},
        "most_cited_patents": []
    }
    
    try:
        citation_counts = []
        
        for patent in key_patents[:10]:  # é™åˆ¶åˆ†ææ•°é‡
            patent_id = patent.get("patent_id", "")
            if patent_id and hasattr(toolkit, 'get_patent_citations'):
                try:
                    citations = toolkit.get_patent_citations(patent_id)
                    cited_by_count = len(citations.get("cited_by", []))
                    citation_counts.append(cited_by_count)
                    
                    patent["citation_count"] = cited_by_count
                    citation_analysis["citation_network"][patent_id] = citations
                    
                except Exception as e:
                    logger.warning(f"è·å–ä¸“åˆ©å¼•ç”¨å¤±è´¥ {patent_id}: {str(e)}")
        
        if citation_counts:
            citation_analysis["total_citations"] = sum(citation_counts)
            citation_analysis["avg_citations"] = sum(citation_counts) / len(citation_counts)
            
            # æ‰¾å‡ºè¢«å¼•ç”¨æœ€å¤šçš„ä¸“åˆ©
            patents_with_citations = [p for p in key_patents if "citation_count" in p]
            patents_with_citations.sort(key=lambda x: x.get("citation_count", 0), reverse=True)
            citation_analysis["most_cited_patents"] = patents_with_citations[:5]
        
        return citation_analysis
        
    except Exception as e:
        logger.error(f"ä¸“åˆ©å¼•ç”¨åˆ†æå¤±è´¥: {str(e)}")
        return citation_analysis


def _analyze_patent_families(toolkit, key_patents: List[Dict]) -> Dict[str, Any]:
    """åˆ†æä¸“åˆ©æ—"""
    family_analysis = {
        "family_count": 0,
        "families": {},
        "largest_family": None
    }
    
    try:
        # ç®€åŒ–çš„ä¸“åˆ©æ—åˆ†æï¼ˆåŸºäºå—è®©äººå’Œå‘æ˜äººï¼‰
        families = defaultdict(list)
        
        for patent in key_patents:
            assignee = patent.get("assignee", "")
            inventor = patent.get("inventor", "")
            
            # ä½¿ç”¨å—è®©äººä½œä¸ºæ—ç¾¤æ ‡è¯†
            if assignee:
                family_key = assignee
                families[family_key].append(patent)
        
        family_analysis["family_count"] = len(families)
        family_analysis["families"] = dict(families)
        
        # æ‰¾å‡ºæœ€å¤§çš„ä¸“åˆ©æ—
        if families:
            largest_family_key = max(families.keys(), key=lambda k: len(families[k]))
            family_analysis["largest_family"] = {
                "assignee": largest_family_key,
                "patent_count": len(families[largest_family_key]),
                "patents": families[largest_family_key]
            }
        
        return family_analysis
        
    except Exception as e:
        logger.error(f"ä¸“åˆ©æ—åˆ†æå¤±è´¥: {str(e)}")
        return family_analysis


def _analyze_competitors(toolkit, patents: List[Dict]) -> Dict[str, Any]:
    """åˆ†æç«äº‰å¯¹æ‰‹"""
    competitor_analysis = {
        "top_assignees": [],
        "top_inventors": [],
        "market_leaders": [],
        "emerging_players": []
    }
    
    try:
        # ç»Ÿè®¡å—è®©äºº
        assignee_counts = defaultdict(int)
        inventor_counts = defaultdict(int)
        
        for patent in patents:
            assignee = patent.get("assignee", "").strip()
            inventor = patent.get("inventor", "").strip()
            
            if assignee:
                assignee_counts[assignee] += 1
            if inventor:
                inventor_counts[inventor] += 1
        
        # æ’åºå¹¶è·å–å‰10
        top_assignees = sorted(assignee_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        top_inventors = sorted(inventor_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        competitor_analysis["top_assignees"] = [
            {"name": name, "patent_count": count} for name, count in top_assignees
        ]
        competitor_analysis["top_inventors"] = [
            {"name": name, "patent_count": count} for name, count in top_inventors
        ]
        
        # è¯†åˆ«å¸‚åœºé¢†å¯¼è€…ï¼ˆä¸“åˆ©æ•°é‡è¶…è¿‡5ä¸ªçš„å—è®©äººï¼‰
        market_leaders = [item for item in competitor_analysis["top_assignees"] if item["patent_count"] >= 5]
        competitor_analysis["market_leaders"] = market_leaders
        
        # è¯†åˆ«æ–°å…´å‚ä¸è€…ï¼ˆä¸“åˆ©æ•°é‡åœ¨2-4ä¸ªçš„å—è®©äººï¼‰
        emerging_players = [item for item in competitor_analysis["top_assignees"] 
                          if 2 <= item["patent_count"] <= 4]
        competitor_analysis["emerging_players"] = emerging_players
        
        return competitor_analysis
        
    except Exception as e:
        logger.error(f"ç«äº‰å¯¹æ‰‹åˆ†æå¤±è´¥: {str(e)}")
        return competitor_analysis


def _analyze_technology_evolution(toolkit, patents: List[Dict]) -> Dict[str, Any]:
    """åˆ†ææŠ€æœ¯æ¼”è¿›"""
    evolution_analysis = {
        "timeline": [],
        "key_milestones": [],
        "trend_analysis": {}
    }
    
    try:
        # æŒ‰å¹´ä»½åˆ†ç»„ä¸“åˆ©
        yearly_patents = defaultdict(list)
        
        for patent in patents:
            pub_date = patent.get("publication_date", "")
            if pub_date:
                try:
                    year = int(pub_date[:4])
                    yearly_patents[year].append(patent)
                except:
                    continue
        
        # æ„å»ºæ—¶é—´çº¿
        for year in sorted(yearly_patents.keys()):
            patents_in_year = yearly_patents[year]
            evolution_analysis["timeline"].append({
                "year": year,
                "patent_count": len(patents_in_year),
                "key_patents": patents_in_year[:3]  # å–å‰3ä¸ªä¸“åˆ©
            })
        
        # è¯†åˆ«å…³é”®é‡Œç¨‹ç¢‘ï¼ˆä¸“åˆ©æ•°é‡çªå¢çš„å¹´ä»½ï¼‰
        timeline = evolution_analysis["timeline"]
        for i in range(1, len(timeline)):
            current_count = timeline[i]["patent_count"]
            previous_count = timeline[i-1]["patent_count"]
            
            if current_count > previous_count * 1.5:  # å¢é•¿è¶…è¿‡50%
                evolution_analysis["key_milestones"].append({
                    "year": timeline[i]["year"],
                    "description": f"ä¸“åˆ©æ•°é‡ä»{previous_count}å¢é•¿åˆ°{current_count}",
                    "growth_rate": (current_count - previous_count) / previous_count
                })
        
        # è¶‹åŠ¿åˆ†æ
        if len(timeline) >= 3:
            recent_years = timeline[-3:]
            total_recent = sum(item["patent_count"] for item in recent_years)
            avg_recent = total_recent / len(recent_years)
            
            evolution_analysis["trend_analysis"] = {
                "recent_activity": "high" if avg_recent > 5 else "moderate" if avg_recent > 2 else "low",
                "avg_patents_per_year": avg_recent,
                "total_years_analyzed": len(timeline)
            }
        
        return evolution_analysis
        
    except Exception as e:
        logger.error(f"æŠ€æœ¯æ¼”è¿›åˆ†æå¤±è´¥: {str(e)}")
        return evolution_analysis


def _assess_infringement_risks(toolkit, high_risk_patents: List[Dict]) -> Dict[str, Any]:
    """è¯„ä¼°ä¾µæƒé£é™©"""
    risk_assessment = {
        "overall_risk_level": "low",
        "high_risk_patents": [],
        "risk_factors": [],
        "mitigation_strategies": []
    }
    
    try:
        if not high_risk_patents:
            return risk_assessment
        
        # åˆ†æé«˜é£é™©ä¸“åˆ©
        critical_patents = []
        
        for patent in high_risk_patents:
            patent_risk = {
                "patent_id": patent.get("patent_id", ""),
                "title": patent.get("title", ""),
                "assignee": patent.get("assignee", ""),
                "risk_score": patent.get("risk_score", 0),
                "risk_factors": []
            }
            
            # è¯†åˆ«å…·ä½“é£é™©å› ç´ 
            if patent.get("status") == "GRANT":
                patent_risk["risk_factors"].append("å·²æˆæƒä¸“åˆ©")
            
            if "system" in patent.get("title", "").lower():
                patent_risk["risk_factors"].append("ç³»ç»Ÿæ€§ä¸“åˆ©ï¼Œè¦†ç›–èŒƒå›´å¹¿")
            
            pub_date = patent.get("publication_date", "")
            if pub_date:
                try:
                    pub_year = int(pub_date[:4])
                    current_year = datetime.now().year
                    if current_year - pub_year <= 3:
                        patent_risk["risk_factors"].append("è¿‘æœŸä¸“åˆ©ï¼Œä¿æŠ¤æœŸé•¿")
                except:
                    pass
            
            critical_patents.append(patent_risk)
        
        risk_assessment["high_risk_patents"] = critical_patents
        
        # è¯„ä¼°æ•´ä½“é£é™©ç­‰çº§
        if len(critical_patents) >= 5:
            risk_assessment["overall_risk_level"] = "high"
        elif len(critical_patents) >= 2:
            risk_assessment["overall_risk_level"] = "medium"
        
        # ç”Ÿæˆé£é™©ç¼“è§£ç­–ç•¥
        risk_assessment["mitigation_strategies"] = [
            "è¿›è¡Œè¯¦ç»†çš„ä¸“åˆ©æƒåˆ©è¦æ±‚åˆ†æ",
            "å¯»æ‰¾ç°æœ‰æŠ€æœ¯è¿›è¡Œä¸“åˆ©æ— æ•ˆæŒ‘æˆ˜",
            "è®¾è®¡è§„é¿æ–¹æ¡ˆé¿å…ä¾µæƒ",
            "è€ƒè™‘ä¸“åˆ©è®¸å¯æˆ–äº¤å‰è®¸å¯",
            "ç›‘æ§ä¸“åˆ©çŠ¶æ€å˜åŒ–"
        ]
        
        return risk_assessment
        
    except Exception as e:
        logger.error(f"ä¾µæƒé£é™©è¯„ä¼°å¤±è´¥: {str(e)}")
        return risk_assessment


def validate_prior_art_research(research_report: str) -> Dict[str, Any]:
    """
    éªŒè¯å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶æŠ¥å‘Šçš„è´¨é‡
    
    Args:
        research_report: å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶æŠ¥å‘Š
        
    Returns:
        dict: éªŒè¯ç»“æœ
    """
    validation_result = {
        "is_valid": True,
        "quality_score": 100,
        "missing_elements": [],
        "suggestions": []
    }
    
    # æ£€æŸ¥å¿…éœ€å…ƒç´ 
    required_elements = [
        "ä¸“åˆ©æ£€ç´¢",
        "ç°æœ‰æŠ€æœ¯",
        "æŠ€æœ¯å‘å±•",
        "ç«äº‰å¯¹æ‰‹",
        "ä¾µæƒé£é™©",
        "ä¸“åˆ©åœ°å›¾"
    ]
    
    for element in required_elements:
        if element not in research_report:
            validation_result["missing_elements"].append(element)
            validation_result["quality_score"] -= 12
    
    # æ£€æŸ¥æŠ¥å‘Šé•¿åº¦
    if len(research_report) < 1200:
        validation_result["suggestions"].append("æŠ¥å‘Šå†…å®¹è¿‡çŸ­ï¼Œå»ºè®®å¢åŠ è¯¦ç»†åˆ†æ")
        validation_result["quality_score"] -= 15
    
    # æ£€æŸ¥æ•°æ®æ”¯æ’‘
    if research_report.count("ä¸“åˆ©") < 5:
        validation_result["suggestions"].append("ä¸“åˆ©æ•°æ®ä¸è¶³ï¼Œå»ºè®®å¢åŠ æ›´å¤šä¸“åˆ©åˆ†æ")
        validation_result["quality_score"] -= 10
    
    # æ£€æŸ¥è¡¨æ ¼å’Œå›¾è¡¨
    if "è¡¨æ ¼" not in research_report and "|" not in research_report:
        validation_result["suggestions"].append("ç¼ºå°‘è¡¨æ ¼å±•ç¤ºï¼Œå»ºè®®æ·»åŠ ä¸“åˆ©æ¸…å•è¡¨æ ¼")
        validation_result["quality_score"] -= 10
    
    # æ£€æŸ¥é£é™©è¯„ä¼°
    if "é£é™©" not in research_report:
        validation_result["suggestions"].append("ç¼ºå°‘é£é™©è¯„ä¼°ï¼Œå»ºè®®æ·»åŠ ä¾µæƒé£é™©åˆ†æ")
        validation_result["quality_score"] -= 15
    
    # åˆ¤æ–­æ˜¯å¦æœ‰æ•ˆ
    if validation_result["quality_score"] < 70:
        validation_result["is_valid"] = False
    
    return validation_result


# æµ‹è¯•å‡½æ•°
def test_prior_art_researcher():
    """æµ‹è¯•å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜...")
    
    # æµ‹è¯•ä¸“åˆ©å»é‡
    test_patents = [
        {"patent_id": "US123456", "title": "Test Patent 1"},
        {"patent_id": "US123456", "title": "Test Patent 1"},  # é‡å¤
        {"patent_id": "US789012", "title": "Test Patent 2"},
    ]
    
    unique_patents = _deduplicate_patents(test_patents)
    print(f"âœ… ä¸“åˆ©å»é‡æµ‹è¯•: {len(test_patents)} -> {len(unique_patents)}")
    
    # æµ‹è¯•æ ¸å¿ƒä¸“åˆ©è¯†åˆ«
    test_patents_with_data = [
        {
            "patent_id": "US123456",
            "title": "Advanced AI System for Medical Diagnosis",
            "assignee": "Google Inc.",
            "publication_date": "2023-01-15",
            "status": "GRANT"
        },
        {
            "patent_id": "US789012", 
            "title": "Simple Method",
            "assignee": "Small Company",
            "publication_date": "2010-01-15",
            "status": "APPLICATION"
        }
    ]
    
    key_patents = _identify_key_patents(test_patents_with_data)
    print(f"âœ… æ ¸å¿ƒä¸“åˆ©è¯†åˆ«: {len(key_patents)} ä¸ªæ ¸å¿ƒä¸“åˆ©")
    
    # æµ‹è¯•æŠ¥å‘ŠéªŒè¯
    test_report = """# å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶æŠ¥å‘Š

## ä¸“åˆ©æ£€ç´¢ç»“æœ
è¿›è¡Œäº†å…¨é¢çš„ä¸“åˆ©æ£€ç´¢...

## ç°æœ‰æŠ€æœ¯åˆ†æ
åˆ†æäº†ç›¸å…³çš„ç°æœ‰æŠ€æœ¯...

## æŠ€æœ¯å‘å±•è¶‹åŠ¿
æŠ€æœ¯å‘å±•å‘ˆç°ä¸Šå‡è¶‹åŠ¿...

## ç«äº‰å¯¹æ‰‹åˆ†æ
ä¸»è¦ç«äº‰å¯¹æ‰‹åŒ…æ‹¬...

## ä¾µæƒé£é™©è¯„ä¼°
è¯†åˆ«äº†å¤šä¸ªé«˜é£é™©ä¸“åˆ©...

## ä¸“åˆ©åœ°å›¾
æ„å»ºäº†æŠ€æœ¯ä¸“åˆ©åœ°å›¾...

| ä¸“åˆ©ID | æ ‡é¢˜ | å—è®©äºº | é£é™©ç­‰çº§ |
|--------|------|--------|----------|
| US123456 | Test Patent | Google | High |
"""
    
    validation = validate_prior_art_research(test_report)
    print(f"âœ… æŠ¥å‘ŠéªŒè¯: è´¨é‡åˆ†æ•° {validation['quality_score']}/100")
    
    if validation["missing_elements"]:
        print(f"âš ï¸ ç¼ºå¤±å…ƒç´ : {validation['missing_elements']}")
    
    if validation["suggestions"]:
        print(f"ğŸ’¡ æ”¹è¿›å»ºè®®: {validation['suggestions']}")
    
    print("ğŸ‰ å…ˆè¡ŒæŠ€æœ¯ç ”ç©¶å‘˜æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_prior_art_researcher() 